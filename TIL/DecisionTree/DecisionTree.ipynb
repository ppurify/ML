{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0️⃣ Overview\n",
    "\n",
    "#### 머신러닝의 학습 방법들\n",
    "\n",
    "- Gradient descent based learning\n",
    "\n",
    "- Probability theory based learning\n",
    "\n",
    "- <span style = 'background-color :#ffdce0' >Information theory based learning</span>  \n",
    "    : 이 방법을 사용하는 것이 Decision Tree  \n",
    "    \n",
    "    Ex) Akinator : 스무고개로 유명인, 캐릭터를 맞추는 게임, 트리형태로 구성되어 있음\n",
    "    \n",
    "- Distance simlarity based learning\n",
    "\n",
    "### Decision Tree Classifier\n",
    "- Data를 가장 잘 구분할 수 있는 Tree를 구성함\n",
    "- Data Y에 따라서 Y를 잘 표현할 수 있는 트리를 구성해서 분류해나가는 기법\n",
    "\n",
    "### Decision Tree 만들기\n",
    "- 어떤 질문(Attribute)이 가장 많은 해답(Y)을 줄 것인가?\n",
    "- 결국 어떤 질문은 답의 모호성을 줄여줄 것인가?\n",
    "- 문제를 통해서 splitting point을 설정  \n",
    "    -> 남은 정보로 splitting point를 설정하는 식\n",
    "\n",
    "Ex) 모호성을 줄여주는 예시  \n",
    "- 답이 딱딱 나누어 떨어짐 => 모호성을 줄여줌\n",
    "- 정확한 성능을 나타낼 수 있는 Tree\n",
    "\n",
    "![ex1](./img/Ex1.PNG)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Information theory - Entropy\n",
    "\n",
    "### Entropy\n",
    "- 목적 달성을 위한 경우의 수를 정량적으로 표현하는 수치  \n",
    "    $\\Rightarrow$ 작을 수록 경우의 수가 적음\n",
    "\n",
    "- Higher $\\uparrow$  Entropy $\\Rightarrow$ Higher uncertainty 불확실성 증가 $\\uparrow$\n",
    "- Lower $\\downarrow$ Entropy $\\Rightarrow$ Lower uncertainty 불확실성 감소 $\\downarrow$\n",
    "\n",
    "<span style = 'color : red'>Entropy가 작으면 얻을 수 있는 정보가 많다, 명확하다</span>\n",
    "\n",
    "- $p_i$가 클수록 사건이 일어날 확률이 명확하다\n",
    "\n",
    "##### $ h(D) = -\\sum_{i=1}^m p_i log_2(p_i)$\n",
    "\n",
    "h(D)의 값이 0에 가까워지면 Entropy는 $\\downarrow$  \n",
    "$\\Rightarrow$ 불확실성이 $\\downarrow$\n",
    "\n",
    "확률이 1 $\\rightarrow$ Entropy = 0  \n",
    "확률이 작을수록 Entropy는 커진다  \n",
    "\n",
    "![Entropy](./img/Entropy.PNG)\n",
    "\n",
    "Ex) $m$ = 1,2,3  \n",
    "$p_1 = 1$ $\\Rightarrow$ $p_2 = p_3 = 0$  \n",
    "\n",
    "$\\Rightarrow$ h(D)에는 $p_1$만 남게되고, $h(D) = - p_1 log_2(p_1)$ \n",
    "\n",
    "이때, $log_2(p_1) = 0, -log_2(p_i) = 0 \\Rightarrow h(D) = 0$  \n",
    "$\\Rightarrow$ Lower Entropy, Lower uncertainty\n",
    "\n",
    "⚠ $p_i$의 값이 다양할 경우 $-log_2(p_i)$의 값은 증가 $\\uparrow$  \n",
    "$\\Rightarrow$ 불확실성 $\\uparrow$\n",
    "\n",
    "Ex) No : 5개, Yes : 9개  \n",
    "\n",
    "$\\rightarrow h(D) = -\\frac{9}{14} log_2\\frac{9}{14} + -\\frac{5}{14} log_2\\frac{5}{14}  \n",
    "= 0.940bits $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9402859586706311"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([9/14, 5/14])\n",
    "y = np.log2(x)\n",
    "\n",
    "-sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Algorithms of Decision Tree\n",
    "- Decision Tree 만드는 것을 시키는 알고리즘 필요\n",
    "\n",
    "- 어떻게 하면 가장 잘 분기(branch)를 만들수 있는가?\n",
    "\n",
    "- Data의 attribute를 기준으로 분기 생성 (어떤기준으로 나눌지)\n",
    "\n",
    "- 어떤 attribute를 기준으로 했을 때 가장 Entropy가 작은가?\n",
    "\n",
    "- 하나를 자른 후에 그 다음은 어떻게 할 것인가? \n",
    "\n",
    "- Decision Tree는 재귀적으로 생김  \n",
    ": 큰 Attribute에서 sub attribute로 나눔\n",
    "\n",
    "- 대산 라벨에 대해 어떤 Attribute가 더 확실한 정보를 제공하고 있는가?로 branch attribute를 선택\n",
    "\n",
    "- 확실한 정보의 선택 기준은 알고리즘 별로 차이 발생\n",
    "\n",
    "- Tree 생성 후 pruning을 통해 Tree generalization 시행  \n",
    "( pruning : Overfitting을 방지하기 위해 가지치기하는 거 )\n",
    "\n",
    "- 일반적으로 효율을 위해 Binary Tree 사용\n",
    "\n",
    "### Decision Tree의 특징\n",
    "- 비교적 간단, <span style = 'color : blue'>직관적으로 결과 표현</span>\n",
    "- 훈련시간이 길고 메모리 공간을 많이 사용\n",
    "- Top-down, Recursive, Divide and Conquer 기법  \n",
    "Top-down : 위에서 아래로 내려가는 구조  \n",
    "Recursive : 재귀적   \n",
    "Divide and Conquer : 나눈 후에 또 다시 나누는 구조  \n",
    "\n",
    "- Greedy 알고리즘 $\\rightarrow$ 부분 최적화  \n",
    ": 전체 optimization이 아닌 현재의 optimization을 찾음  \n",
    "Ex) 주황에서도 최적화, 파랑에서도 최적화됨\n",
    "\n",
    "![ex2](./img/Ex2.png)\n",
    "\n",
    "### Decision Tree의 장점\n",
    "- 트리의 상단 부분 attribute들이 가장 중요한 예측변수  \n",
    "$\\Rightarrow$ attribute 선택 기법으로도 활용 가능\n",
    "\n",
    "- Attribute의 scaling 필요 X\n",
    "- 관측치의 절대값이 아닌 순서가 중요  \n",
    "$\\Rightarrow$ Outlier에 이점  \n",
    "(Outlier : 정규분포에서 양극단에 있는 거)\n",
    "\n",
    "- 자동적 변수 부분 선택 $\\Leftarrow$ Tree Pruning \n",
    "\n",
    "### Algorithms of Decision Tree\n",
    "- 크게 두가지 형태의 decision tree 알고리즘 존재  \n",
    "    : 1. ID3, 2. CART\n",
    "\n",
    "- 알고리즘 별 attribute branch 방법이 다름\n",
    "- ID3 $\\rightarrow$ C4.5(Ross Quinlan), CART\n",
    "- 연속형 변수를 위한 regression tree도 존재 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('purify')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa9987605a02e7ab2193e9cae0389bc7af93e15a024bcfcd5136c75f68f712e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
