{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "### 0️⃣ Overview\n",
    "Supervised Learning\n",
    "\n",
    "- Gradient descent based learning  \n",
    "- probability theory based learning\n",
    "- Information theory based learning\n",
    "- Distance similarity based learning\n",
    "\n",
    "#### 1️⃣ Gradient descent based learning  \n",
    ": 실제 값과 학습된 모델 예측치의 <span style = 'background-color:#fff5b1'>오차를 최소화</span>   \n",
    "<span style= 'color:#008000'>모델의 최적 parameter 찾기가 목적</span>  \n",
    "gradient descent 활용\n",
    "\n",
    "예측값 : $\\hat{y}$  \n",
    "오차 : $\\epsilon$ (예측 불가)\n",
    "\n",
    "좋은 선분을 찾는 방법 : algorithm  \n",
    "선분을 찾는 최종 수식 : model\n",
    "\n",
    "<span style= 'color:#008000'>예측 함수와 실제 값의 오차를 줄여보자 !</span>  \n",
    "\n",
    "- 오차의 합 : 오차는 양수 또는 음수의 형태를 가질 수 있다 $\\Rightarrow$ 상쇄될 수 있음   \n",
    "$\\Rightarrow$ 오차의 합을 제곱의 합으로 변환  \n",
    "\n",
    "모델을 표현하는 언어 사용할 거임\n",
    "\n",
    "#### Squared Error를 최소화 할 수 있는 weight 값의 발견 (2가지 방식)\n",
    "\n",
    "- 최소 또는 최대의 문제 $\\Rightarrow$ 편미분으로 해결\n",
    "\n",
    "### 2️⃣ Cost Function\n",
    "$f(x) = h_\\theta(x) = \\hat{y}$  \n",
    "$\\theta $ : 가중치     \n",
    "Ex) $ h_\\theta(x) = ax + b \\Rightarrow a: \\theta_0, b: \\theta_1$  \n",
    "- 예측 함수를 가설 함수라고 부를 예정 \n",
    "\n",
    "- 실제값과 가설함수의 차이  \n",
    "\n",
    "$J(w_0,w_1) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta x^{i} -y^{i})^2$  \n",
    "\n",
    "Cost function 이라고 부를 예정  \n",
    "Cost function의 최소화를 위한 weight($\\theta$) 값  \n",
    "\n",
    "\n",
    "#### $J(w_0,w_1) = \\frac{1}{2m} \\sum_{i=1}^m (w_1 x^{(i)} + w_0 - y^{(i)})^2$\n",
    "- $w_0$에 관해 미분\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)}) $\n",
    "- $w_1$에 관해 미분\n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)}) x^{(i)} $\n",
    "\n",
    "#### Weights의 최적값 컴퓨터가 찾는 방법\n",
    "1. 연립방정식 풀기 (normal equation)\n",
    "2. <span style = 'background-color:#fff5b1'>gradient descent</span>\n",
    "\n",
    "### 3️⃣ Normal equation\n",
    "Cost Fuction을 최소화 하는 방법 중 하나\n",
    "\n",
    "$y= Xw$  \n",
    "\n",
    "$ y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ y^{(4)} \\\\ y^{(5)} \\\\ \\end{bmatrix}$\n",
    "$ X = \\begin{bmatrix} 1 & x^{(1)} \\\\ 1 & x^{(2)} \\\\ 1 & x^{(3)} \\\\ 1 & x^{(4)} \\\\ 1 & x^{(5)} \\\\ \\end{bmatrix}$\n",
    "$w = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$(X^T X)\\hat{w} = x^T y$\n",
    "\n",
    "$(X^T X) = \\begin{bmatrix} m & \\Sigma x^{(i)} \\\\ \\Sigma x^{(i)} & \\Sigma (x^{(i)})^2 \\\\ \\end{bmatrix}$  \n",
    "  \n",
    "$X^Ty = \\begin{bmatrix} \\Sigma y^{(i)} \\\\ \\Sigma x^{(i)}y^{(i)} \\\\ \\end{bmatrix}$\n",
    "\n",
    "$ \\hat{w} = (X^TX)^{-1}X^T y = \\begin{bmatrix} \\hat{w_0} \\\\ \\hat{w_1} \\\\ \\end{bmatrix}$\n",
    "\n",
    "$ \\hat{w_1} = \\frac{\\Sigma x^{(i)}y^{(i)} - m \\bar{x}\\bar{y}}{\\Sigma (x^{(i)}-\\bar{x})^2}$\n",
    "\n",
    "$\\hat{w_0} = \\bar{y} - \\hat{w_1}\\bar{x}$\n",
    "\n",
    "- 여러개의 변수일 경우 $X^TX$가 확대됨\n",
    "\n",
    "<span style = 'color:red'>결론</span>\n",
    "\n",
    "$ \\hat{w} = (X^T X)^{-1} X^T y $\n",
    "\n",
    "- $X^TX$의 역행렬이 존재할 때 사용\n",
    "- Iteration 등 사용자 지정 parameter가 없음\n",
    "- Feature가 많으면 계산 속도 느려짐 (지금은 단점이 아님!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "730b21a53400ae1ba7176fa0274ae345f408e033addad13218c73726d872c853"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
