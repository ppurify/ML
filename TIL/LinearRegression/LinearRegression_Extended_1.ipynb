{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Extended\n",
    "### 0ï¸âƒ£ Gradient Descent\n",
    "\n",
    "### âœ” Gradient Descent \n",
    "$x_{new} = x_{old} - \\alpha \\times (2x_{old})$\n",
    "\n",
    "### âœ” Full-batch Gradient Descent\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1)$  \n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})$   \n",
    "  \n",
    "$\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})x^{(i)}$   \n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_n} = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}w - y^{(i)})x_n$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local Optimize point(ì§€ì—­ ìµœì ì )ì— ë¹ ì§€ì§€ì•Šê²Œ ë°©ì§€,  \n",
    "í—¤ì–´ë‚˜ì˜¤ê¸° ìœ„í•´ ëª¨ë“  Dataë¥¼ ëª¨ì•„ë†¨ë‹¤ê°€ í•œêº¼ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°©ë²•\n",
    "\n",
    "- GDëŠ” 1ê°œì˜ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¯¸ë¶„\n",
    "- But ì¼ë°˜ì ìœ¼ë¡œ GD = (full) batch GDë¼ê³  ê°€ì •\n",
    "- ëª¨ë“  ë°ì´í„° ì…‹ìœ¼ë¡œ í•™ìŠµ\n",
    "- ì—…ë°ì´íŠ¸ ê°ì†Œ $\\rightarrow$ Single GDë³´ë‹¤ ê³„ì‚°ìƒ íš¨ìœ¨ì (ì†ë„) ê°€ëŠ¥\n",
    "- ì•ˆì •ì ì¸ Cost í•¨ìˆ˜ ìˆ˜ë ´\n",
    "- ì§€ì—­ ìµœì í™” ê°€ëŠ¥\n",
    "- ë©”ëª¨ë¦¬ ë¬¸ì œ (ex - 30ì–µê°œì˜ Dataë¥¼ í•œë²ˆì— ì²˜ë¦¬í•  ê²½ìš°)\n",
    "- ëŒ€ê·œëª¨ Dataset $\\rightarrow$ ëª¨ë¸/Parameter ì—…ë°ì´íŠ¸ê°€ ëŠë ¤ì§"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ï¸âƒ£ Mini-batch (Stochastic) Gradient Descent\n",
    "- ì „ì²´ ë°ì´í„°ì—ì„œ ì¼ë¶€ ë½‘ì•„ì„œ ì‚¬ìš©\n",
    "- í•œë²ˆì˜ ì¼ì •ëŸ‰ì˜ ë°ì´í„°ë¥¼ ëœë¤í•˜ê²Œ ë½‘ì•„ì„œ í•™ìŠµ\n",
    "- SGDì™€ Full-Batch GDë¥¼ í˜¼í•©í•œ ê¸°ë²•\n",
    "- ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì“°ì´ëŠ” ê¸°ë²•\n",
    "- GD $\\rightarrow$ SGD $\\rightarrow$ Mini SGD \n",
    "\n",
    "### âœ” Epoch & Batch-size\n",
    "- ì „ì²´ ë°ì´í„°ê°€ í•œë²ˆ Training Dataì— ë“¤ì–´ê°ˆ ë•Œ ì¹´ìš´íŒ…\n",
    "- Full-batchë¥¼ në²ˆ ì‹¤í–‰í•˜ë©´ n epoch\n",
    "- Batch-size : í•œë²ˆì— í•™ìŠµë˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ , ì£¼ë¡œ $2^n$ ì‚¬ìš©  \n",
    "ex) ì´ ë°ì´í„°ê°€ 500ê°œì¼ ë•Œ, 100ê°œì”© ì˜ë¼ì„œ 5ë²ˆ í•™ìŠµ $\\Rightarrow$ Batch-size : 100\n",
    "\n",
    "- ì´ 5,120ê°œì˜ Training Dataì— 512 batch-sizeë©´ ëª‡ë²ˆ í•™ìŠµì„ í•´ì•¼ 1 epochì´ ë˜ëŠ”ê°€?  \n",
    " $\\Rightarrow$ 10ë²ˆ í•™ìŠµ\n",
    "\n",
    "- Hyper-parameter : Batch-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:red'>ê²°ë¡ </span>\n",
    "#### <span style = 'color:green'>GD</span>\n",
    ": í•œ ì ì”© ë‚´ë ¤ê°, ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì´ ê°€ë„ë¡\n",
    "- DataSetì´ ì‘ì„ ê²½ìš° ì‚¬ìš©\n",
    "\n",
    "#### <span style = 'color:green'>Full-Batch GD</span>\n",
    ": ë°ì´í„° ì „ì²´ ì‚¬ìš©, ì´ë¥¼ 1 epochì´ë¼ ë¶€ë¦„\n",
    "- DataSetì´ í° ê²½ìš° ì‚¬ìš©\n",
    "\n",
    "Linear Regressionì—ì„œ GDì™€ Full-Batch GDëŠ” í° ì°¨ì´ ì—†ìŒ\n",
    "\n",
    "#### <span style = 'color:green'>SGD</span>\n",
    ": ì—¬ëŸ¬ ì ì„ ëŒì•„ê°€ë©´ì„œ ê¸°ìš¸ê¸° êµ¬í•´ì„œ ë‚´ë ¤ê°\n",
    "\n",
    "#### <span style = 'color:green'>Mini-Batch GD</span>\n",
    ": êµ¬ê°„ì„ ì˜ë¼ì„œ í•œë²ˆì— updateí•´ê°€ë©° ê¸°ìš¸ê¸°ê°€ ë‚´ë ¤ê°\n",
    "- ë”¥ëŸ¬ë‹ì—ì„œ ì‚¬ìš©í•  ê²½ìš°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GD](./img/GD.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2ï¸âƒ£ SGD implementation issues\n",
    "- Data ë³µì¡í•  ì‹œ SGD ì‚¬ìš©\n",
    "#### âœ” SGDë¥¼ êµ¬í˜„í•  ë•Œ ìƒê°í•´ë´ì•¼ í•  ì¼ ë“¤\n",
    "\n",
    "### 1.Mini-Batch SGD\n",
    "- epoch : ì‚¬ëŒì´ ì§€ì •í•´ì£¼ëŠ” Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ì „ì²´ Epochì´ iteration ë˜ëŠ” íšŸìˆ˜\n",
    "# for epoch in range(epoches):\n",
    "#     X_copy = np.copy(X)\n",
    "\n",
    "#     # SGD ì—¬ë¶€ \n",
    "#     # -> SGDì¼ ê²½ìš° shuffle \n",
    "#     if is_SGD:\n",
    "#         # Data ì„ì–´ì£¼ëŠ” ì‘ì—…\n",
    "#         np.random.shuffle(X_copy)\n",
    "    \n",
    "#     # í•œë²ˆì— ì²˜ë¦¬í•˜ëŠ” BATCH_SIZE, ëª«\n",
    "#     batch = len(X_copy) // BATCH_SIZE\n",
    "#     for batch_count in range(batch):\n",
    "#         # BATCH_SIZE í¬ê¸° ë§Œí¼ X_batch ìƒì„±, EX) BATCH_SIZE : 100ì¼ ê²½ìš° -> 0 ~ 100, 100~200,...\n",
    "#         X_batch = np.copy(X_copy [batch_count * BATCH_SIZE : (batch_count + 1) * BATCH_SIZE ] )\n",
    "    \n",
    "#     # Do weight Update\n",
    "#     print('Number of epoch : %d' % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convergence process\n",
    "![ConvergenceProcess](./img/ConvergenceProcess.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets.samples_generator import make_regression\n",
    "# X, y = make_regression(n_samples = 1000, n_features = 1, noise = 10, random_state = 42)\n",
    "# X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ” Learning RateëŠ” ì¼ì •í•´ì•¼ í•˜ëŠ”ê°€?\n",
    "- Learning RateëŠ” ì ì  ì‘ì•„ì§€ë©´ ì¢‹ì§€ ì•Šì„ê¹Œ?\n",
    "- Learning Rate ì¡°ì • í•„ìš”\n",
    "\n",
    "#### Learning-Rate decay (ê°ì†Œ)\n",
    "- ì¼ì •í•œ ì£¼ê¸°ë¡œ Learning Rateã…‡ë¥´ ê°ì†Œì‹œí‚¤ëŠ” ë°©ë²•\n",
    "- íŠ¹ì • epochë§ˆë‹¤ Learning Rate ê°ì†Œ\n",
    "```\n",
    "self._eta0 = self._eta0 * self._learning_rate_decay\n",
    "```\n",
    "- Hyper-parameter ì„¤ì • ì–´ë ¤ì›€\n",
    "- t : epoch\n",
    "- ì§€ìˆ˜ê°ì†Œ $\\alpha = \\alpha_{0}e^{-kt}$\n",
    "\n",
    "- 1/t ê°ì†Œ $\\alpha = \\frac{\\alpha_0}{(1+kt)}$\n",
    "\n",
    "### âœ” ì¢…ë£Œ ì¡°ê±´ ì„¤ì •\n",
    "- SGD ê³¼ì •ì—ì„œ íŠ¹ì • ê°’ì´í•˜ë¡œ cost functionì´ ì¤„ì–´ë“¤ì§€ ì•Šì„ ê²½ìš° GDë¥¼ ë©ˆì¶”ëŠ” ë°©ë²•\n",
    "\n",
    "- ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•ŠëŠ”/í•„ìš”ì—†ëŠ” ì—°ì‚°ì„ ë°©ì§€\n",
    "\n",
    "- ì¢…ë£Œì¡°ê±´ì„ ì„¤ì • : tol > (ì´ë²ˆ cost functoin)loss - (ì´ì „ cost function) previous_loss\n",
    "\n",
    "- tolì€ Hyper-parameterë¡œ ì‚¬ëŒ ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3ï¸âƒ£ Overfitting\n",
    ": í•™ìŠµ ë°ì´í„° ê³¼ë‹¤ ìµœì í™” $\\rightarrow$ ìƒˆë¡œìš´ ë°ì´í„°ì˜ ì˜ˆì¸¡ $\\downarrow$\n",
    "- Training Dataì— ë„ˆë¬´ ì˜ ë§ì¶”ì–´ì ¸ ìˆëŠ” ê²½ìš°\n",
    "\n",
    "![fitting](./img/Overfitting.PNG)\n",
    "\n",
    "- Underfitting : High bias\n",
    "- Overfitting : High variance\n",
    "\n",
    "#### ë³´ë‹¤ ì ì€ ìˆ˜ì˜ ë…¼ë¦¬(weight)ë¡œ ì„¤ëª…ì´ ê°€ëŠ¥í•œ ê²½ìš°, ë§ì€ ìˆ˜ì˜ ë…¼ë¦¬ë¥¼ ì„¸ìš°ì§€ ë§ë¼ - Occam's razor\n",
    "\n",
    "### âœ” Bias - Variance tradeoff\n",
    "#### 1. High bias \n",
    "- ì›ë˜ ëª¨ë¸ì— ë§ì´ ë–¨ì–´ì§\n",
    "- ì˜ëª»ëœ ë°ì´í„°ë§Œ ê³„ì† í•™ìŠµí•¨  \n",
    "$\\Rightarrow$ ì˜ëª»ëœ Weightë§Œ Update\n",
    "\n",
    "#### 2. High variance\n",
    "- ëª¨ë“  ë°ì´í„°ì— ë¯¼ê°í•˜ê²Œ í•™ìŠµ\n",
    "- Error ê³ ë ¤ X  \n",
    "$\\Rightarrow$ ëª¨ë“  Weightê°€ Update\n",
    "\n",
    "### âœ” Train-Test Error\n",
    "- Overfitting  \n",
    "\n",
    "![Error](./img/Train-Test%20Error.PNG)\n",
    "\n",
    "### âœ” Overcoming Overfitting\n",
    "- <span style = 'background-color:#fff5b1'>ë” ë§ì€ ë°ì´í„°ë¥¼ í™œìš© (ì‰½ì§€ ì•Šë‹¤, í•œê³„ ìˆìŒ)</span>\n",
    "- Featureì˜ ê°œìˆ˜ë¥¼ ì¤„ì„\n",
    "- ì ì ˆíˆ Parameter ì„ ì •\n",
    "- <span style = 'color:blue'>Regularization</span>  \n",
    ": $\\theta$ë¥¼ 0ì— ê°€ê¹ë„ë¡ í•˜ëŠ” ê¸°ë²•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4ï¸âƒ£ Regularizaion\n",
    ": <span style = 'color:green'>Penalty</span>ë¥¼ ì¤˜ì„œ Weight ì¤„ì´ëŠ” ê¸°ë²•\n",
    "- Penalized Linear Regressionë¼ê³ ë„ í•¨\n",
    "- 1. L1 : Manhattan Distance\n",
    "- 2. L2 : Euclidean Distance\n",
    "\n",
    "#### $J(w_0,w_1) = \\frac{1}{2m} \\sum_{i=1}^m (w_1 x^{(i)} + w_0 - y^{(i)})^2$\n",
    "#### $\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})$\n",
    "#### $\\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)}) x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <span style = 'background-color:#fff5b1'>L2 Regularizaion</span>\n",
    "#### âœ” ê¸°ì¡´ Cost function L2(norm) penalty termì„ ì¶”ê°€, Overfitting ë˜ì§€ ì•Šë„ë¡\n",
    "##### $h_{\\theta}(x^{(i)}) = w_1 x^{(i)} + w_0$\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\theta_{j}^2$\n",
    "##### $\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_{0}^{(i)} $ \n",
    "##### $\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_j ]$   \n",
    "$\\downarrow$\n",
    "##### $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_{j}^{(i)}$\n",
    "\n",
    "- $j \\in {1,2,...n} $\n",
    "- $\\lambda$ : Hyper-Parameter\n",
    "- $\\lambda \\uparrow$  $\\Rightarrow$ $\\theta \\downarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœ” norm : ë²¡í„°ì˜ ê¸¸ì´ or í¬ê¸°ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•  \n",
    "##### $\\parallel (\\theta) \\parallel _{2}^{2}$\n",
    "<span style = 'color:green'>L2ëŠ” Euclidean Distance</span> ì›ì ì—ì„œ ë²¡í„° ì¢Œí‘œê¹Œì§€ì˜ ê±°ë¦¬\n",
    "\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\theta_{j}^2$\n",
    "$\\downarrow$\n",
    "\n",
    "##### $J(\\theta) = y^Ty - 2\\theta^TX^Ty + \\theta^T(X^TX + \\lambda I)\\theta$\n",
    "$\\downarrow$ ë¯¸ë¶„\n",
    "\n",
    "##### $\\frac{\\partial J(\\theta)}{\\partial \\theta} = - 2 X^T y + 2(X^TX + \\lambda I)\\theta$\n",
    "$\\downarrow$\n",
    "\n",
    "$(X^TX + \\lambda I)\\theta = X^T y \\rightarrow$ <span style = 'color:red'>$\\hat{\\theta} = (X^TX + \\lambda I)^{-1}X^T y$</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. L1 regularization (Lasso Regression)\n",
    ": <span style = 'color:green'>L1ì€ Manhattan Distance</span> ì›ì ì—ì„œ ë²¡í„° ì¢Œí‘œê¹Œì§€ì˜ ê±°ë¦¬\n",
    "\n",
    "#### âœ” ê¸°ì¡´ cost function L1(norm) penalty term ì¶”ê°€\n",
    "\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\left\\lvert \\theta_{j} \\right\\lvert$\n",
    "\n",
    "#### âœ” norm : ë²¡í„°ì˜ ê¸¸ì´ í˜¹ì€ í¬ê¸°ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²• \n",
    "\n",
    "##### $\\parallel x \\parallel _1$ := $\\sum_{i=1}^n \\left\\lvert x_{i} \\right\\lvert$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ” L1, L2 ì°¨ì´\n",
    "ğŸ”¹ L1\n",
    "- unstable solution\n",
    "- 'One or More' solution\n",
    "- Sparse solution   \n",
    ": ì–´ë–¤ weightì˜ ê°’ì´ 0ì´ ë  ìˆ˜ ìˆìŒ\n",
    "- Feature selection\n",
    "\n",
    "ğŸ”¹ L2\n",
    "- Stable solution\n",
    "- Only one solution\n",
    "- Non-sparse solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "730b21a53400ae1ba7176fa0274ae345f408e033addad13218c73726d872c853"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
