{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Extended\n",
    "### 0️⃣ Gradient Descent\n",
    "\n",
    "### ✔ Gradient Descent \n",
    "$x_{new} = x_{old} - \\alpha \\times (2x_{old})$\n",
    "\n",
    "### ✔ Full-batch Gradient Descent\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0,\\theta_1)$  \n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})$   \n",
    "  \n",
    "$\\frac{\\partial J}{\\partial w_1} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})x^{(i)}$   \n",
    "\n",
    "$\\frac{\\partial J}{\\partial w_n} = \\frac{1}{m} \\sum_{i=1}^m (x^{(i)}w - y^{(i)})x_n$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local Optimize point(지역 최적점)에 빠지지않게 방지,  \n",
    "헤어나오기 위해 모든 Data를 모아놨다가 한꺼번에 처리하는 방법\n",
    "\n",
    "- GD는 1개의 데이터를 기준으로 미분\n",
    "- But 일반적으로 GD = (full) batch GD라고 가정\n",
    "- 모든 데이터 셋으로 학습\n",
    "- 업데이트 감소 $\\rightarrow$ Single GD보다 계산상 효율적(속도) 가능\n",
    "- 안정적인 Cost 함수 수렴\n",
    "- 지역 최적화 가능\n",
    "- 메모리 문제 (ex - 30억개의 Data를 한번에 처리할 경우)\n",
    "- 대규모 Dataset $\\rightarrow$ 모델/Parameter 업데이트가 느려짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1️⃣ Mini-batch (Stochastic) Gradient Descent\n",
    "- 전체 데이터에서 일부 뽑아서 사용\n",
    "- 한번의 일정량의 데이터를 랜덤하게 뽑아서 학습\n",
    "- SGD와 Full-Batch GD를 혼합한 기법\n",
    "- 가장 일반적으로 많이 쓰이는 기법\n",
    "- GD $\\rightarrow$ SGD $\\rightarrow$ Mini SGD \n",
    "\n",
    "### ✔ Epoch & Batch-size\n",
    "- 전체 데이터가 한번 Training Data에 들어갈 때 카운팅\n",
    "- Full-batch를 n번 실행하면 n epoch\n",
    "- Batch-size : 한번에 학습되는 데이터의 개수 , 주로 $2^n$ 사용  \n",
    "ex) 총 데이터가 500개일 때, 100개씩 잘라서 5번 학습 $\\Rightarrow$ Batch-size : 100\n",
    "\n",
    "- 총 5,120개의 Training Data에 512 batch-size면 몇번 학습을 해야 1 epoch이 되는가?  \n",
    " $\\Rightarrow$ 10번 학습\n",
    "\n",
    "- Hyper-parameter : Batch-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:red'>결론</span>\n",
    "#### <span style = 'color:green'>GD</span>\n",
    ": 한 점씩 내려감, 기울기가 0에 가까이 가도록\n",
    "- DataSet이 작을 경우 사용\n",
    "\n",
    "#### <span style = 'color:green'>Full-Batch GD</span>\n",
    ": 데이터 전체 사용, 이를 1 epoch이라 부름\n",
    "- DataSet이 큰 경우 사용\n",
    "\n",
    "Linear Regression에서 GD와 Full-Batch GD는 큰 차이 없음\n",
    "\n",
    "#### <span style = 'color:green'>SGD</span>\n",
    ": 여러 점을 돌아가면서 기울기 구해서 내려감\n",
    "\n",
    "#### <span style = 'color:green'>Mini-Batch GD</span>\n",
    ": 구간을 잘라서 한번에 update해가며 기울기가 내려감\n",
    "- 딥러닝에서 사용할 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GD](./img/GD.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2️⃣ SGD implementation issues\n",
    "- Data 복잡할 시 SGD 사용\n",
    "#### ✔ SGD를 구현할 때 생각해봐야 할 일 들\n",
    "\n",
    "### 1.Mini-Batch SGD\n",
    "- epoch : 사람이 지정해주는 Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전체 Epoch이 iteration 되는 횟수\n",
    "# for epoch in range(epoches):\n",
    "#     X_copy = np.copy(X)\n",
    "\n",
    "#     # SGD 여부 \n",
    "#     # -> SGD일 경우 shuffle \n",
    "#     if is_SGD:\n",
    "#         # Data 섞어주는 작업\n",
    "#         np.random.shuffle(X_copy)\n",
    "    \n",
    "#     # 한번에 처리하는 BATCH_SIZE, 몫\n",
    "#     batch = len(X_copy) // BATCH_SIZE\n",
    "#     for batch_count in range(batch):\n",
    "#         # BATCH_SIZE 크기 만큼 X_batch 생성, EX) BATCH_SIZE : 100일 경우 -> 0 ~ 100, 100~200,...\n",
    "#         X_batch = np.copy(X_copy [batch_count * BATCH_SIZE : (batch_count + 1) * BATCH_SIZE ] )\n",
    "    \n",
    "#     # Do weight Update\n",
    "#     print('Number of epoch : %d' % epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convergence process\n",
    "![ConvergenceProcess](./img/ConvergenceProcess.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets.samples_generator import make_regression\n",
    "# X, y = make_regression(n_samples = 1000, n_features = 1, noise = 10, random_state = 42)\n",
    "# X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✔ Learning Rate는 일정해야 하는가?\n",
    "- Learning Rate는 점점 작아지면 좋지 않을까?\n",
    "- Learning Rate 조정 필요\n",
    "\n",
    "#### Learning-Rate decay (감소)\n",
    "- 일정한 주기로 Learning Rateㅇ르 감소시키는 방법\n",
    "- 특정 epoch마다 Learning Rate 감소\n",
    "```\n",
    "self._eta0 = self._eta0 * self._learning_rate_decay\n",
    "```\n",
    "- Hyper-parameter 설정 어려움\n",
    "- t : epoch\n",
    "- 지수감소 $\\alpha = \\alpha_{0}e^{-kt}$\n",
    "\n",
    "- 1/t 감소 $\\alpha = \\frac{\\alpha_0}{(1+kt)}$\n",
    "\n",
    "### ✔ 종료 조건 설정\n",
    "- SGD 과정에서 특정 값이하로 cost function이 줄어들지 않을 경우 GD를 멈추는 방법\n",
    "\n",
    "- 성능이 좋아지지 않는/필요없는 연산을 방지\n",
    "\n",
    "- 종료조건을 설정 : tol > (이번 cost functoin)loss - (이전 cost function) previous_loss\n",
    "\n",
    "- tol은 Hyper-parameter로 사람 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3️⃣ Overfitting\n",
    ": 학습 데이터 과다 최적화 $\\rightarrow$ 새로운 데이터의 예측 $\\downarrow$\n",
    "- Training Data에 너무 잘 맞추어져 있는 경우\n",
    "\n",
    "![fitting](./img/Overfitting.PNG)\n",
    "\n",
    "- Underfitting : High bias\n",
    "- Overfitting : High variance\n",
    "\n",
    "#### 보다 적은 수의 논리(weight)로 설명이 가능한 경우, 많은 수의 논리를 세우지 말라 - Occam's razor\n",
    "\n",
    "### ✔ Bias - Variance tradeoff\n",
    "#### 1. High bias \n",
    "- 원래 모델에 많이 떨어짐\n",
    "- 잘못된 데이터만 계속 학습함  \n",
    "$\\Rightarrow$ 잘못된 Weight만 Update\n",
    "\n",
    "#### 2. High variance\n",
    "- 모든 데이터에 민감하게 학습\n",
    "- Error 고려 X  \n",
    "$\\Rightarrow$ 모든 Weight가 Update\n",
    "\n",
    "### ✔ Train-Test Error\n",
    "- Overfitting  \n",
    "\n",
    "![Error](./img/Train-Test%20Error.PNG)\n",
    "\n",
    "### ✔ Overcoming Overfitting\n",
    "- <span style = 'background-color:#fff5b1'>더 많은 데이터를 활용 (쉽지 않다, 한계 있음)</span>\n",
    "- Feature의 개수를 줄임\n",
    "- 적절히 Parameter 선정\n",
    "- <span style = 'color:blue'>Regularization</span>  \n",
    ": $\\theta$를 0에 가깝도록 하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4️⃣ Regularizaion\n",
    ": <span style = 'color:green'>Penalty</span>를 줘서 Weight 줄이는 기법\n",
    "- Penalized Linear Regression라고도 함\n",
    "- 1. L1 : Manhattan Distance\n",
    "- 2. L2 : Euclidean Distance\n",
    "\n",
    "#### $J(w_0,w_1) = \\frac{1}{2m} \\sum_{i=1}^m (w_1 x^{(i)} + w_0 - y^{(i)})^2$\n",
    "#### $\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)})$\n",
    "#### $\\frac{\\partial J}{\\partial w_{1}} = \\frac{1}{m} \\sum_{i=1}^m (w_{1} x^{(i)} + w_{0} - y^{(i)}) x^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <span style = 'background-color:#fff5b1'>L2 Regularizaion</span>\n",
    "#### ✔ 기존 Cost function L2(norm) penalty term을 추가, Overfitting 되지 않도록\n",
    "##### $h_{\\theta}(x^{(i)}) = w_1 x^{(i)} + w_0$\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\theta_{j}^2$\n",
    "##### $\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)}) x_{0}^{(i)} $ \n",
    "##### $\\theta_j := \\theta_j - \\alpha [\\frac{1}{m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_j ]$   \n",
    "$\\downarrow$\n",
    "##### $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)}) x_{j}^{(i)}$\n",
    "\n",
    "- $j \\in {1,2,...n} $\n",
    "- $\\lambda$ : Hyper-Parameter\n",
    "- $\\lambda \\uparrow$  $\\Rightarrow$ $\\theta \\downarrow$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ✔ norm : 벡터의 길이 or 크기를 측정하는 방법  \n",
    "##### $\\parallel (\\theta) \\parallel _{2}^{2}$\n",
    "<span style = 'color:green'>L2는 Euclidean Distance</span> 원점에서 벡터 좌표까지의 거리\n",
    "\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\theta_{j}^2$\n",
    "$\\downarrow$\n",
    "\n",
    "##### $J(\\theta) = y^Ty - 2\\theta^TX^Ty + \\theta^T(X^TX + \\lambda I)\\theta$\n",
    "$\\downarrow$ 미분\n",
    "\n",
    "##### $\\frac{\\partial J(\\theta)}{\\partial \\theta} = - 2 X^T y + 2(X^TX + \\lambda I)\\theta$\n",
    "$\\downarrow$\n",
    "\n",
    "$(X^TX + \\lambda I)\\theta = X^T y \\rightarrow$ <span style = 'color:red'>$\\hat{\\theta} = (X^TX + \\lambda I)^{-1}X^T y$</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. L1 regularization (Lasso Regression)\n",
    ": <span style = 'color:green'>L1은 Manhattan Distance</span> 원점에서 벡터 좌표까지의 거리\n",
    "\n",
    "#### ✔ 기존 cost function L1(norm) penalty term 추가\n",
    "\n",
    "##### $J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2}\\sum_{j=1}^n \\left\\lvert \\theta_{j} \\right\\lvert$\n",
    "\n",
    "#### ✔ norm : 벡터의 길이 혹은 크기를 측정하는 방법 \n",
    "\n",
    "##### $\\parallel x \\parallel _1$ := $\\sum_{i=1}^n \\left\\lvert x_{i} \\right\\lvert$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✔ L1, L2 차이\n",
    "🔹 L1\n",
    "- unstable solution\n",
    "- 'One or More' solution\n",
    "- Sparse solution   \n",
    ": 어떤 weight의 값이 0이 될 수 있음\n",
    "- Feature selection\n",
    "\n",
    "🔹 L2\n",
    "- Stable solution\n",
    "- Only one solution\n",
    "- Non-sparse solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "730b21a53400ae1ba7176fa0274ae345f408e033addad13218c73726d872c853"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
